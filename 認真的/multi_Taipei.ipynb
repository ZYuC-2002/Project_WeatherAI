{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Predicting the price of Bitcoin with multivariate Pytorch LSTMs - revise](https://charlieoneill.medium.com/predicting-the-price-of-bitcoin-with-multivariate-pytorch-lstms-695bc294130)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\莊于潔\\AppData\\Local\\Temp\\ipykernel_4964\\2192393335.py:14: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "'''Train with PyTorch.'''\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "import torch.utils.data as data\n",
    "\n",
    "# SciKit\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "# Python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>觀測時間(hour)</th>\n",
       "      <th>氣溫(℃)</th>\n",
       "      <th>相對溼度(%)</th>\n",
       "      <th>露點溫度(℃)</th>\n",
       "      <th>測站氣壓(hPa)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010/1/1 00:00</td>\n",
       "      <td>13.7</td>\n",
       "      <td>69.0</td>\n",
       "      <td>8.2</td>\n",
       "      <td>1021.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010/1/1 01:00</td>\n",
       "      <td>13.8</td>\n",
       "      <td>69.0</td>\n",
       "      <td>8.3</td>\n",
       "      <td>1021.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010/1/1 02:00</td>\n",
       "      <td>13.9</td>\n",
       "      <td>69.0</td>\n",
       "      <td>8.4</td>\n",
       "      <td>1020.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010/1/1 03:00</td>\n",
       "      <td>13.9</td>\n",
       "      <td>69.0</td>\n",
       "      <td>8.4</td>\n",
       "      <td>1019.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010/1/1 04:00</td>\n",
       "      <td>14.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>8.5</td>\n",
       "      <td>1019.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       觀測時間(hour)  氣溫(℃)  相對溼度(%)  露點溫度(℃)  測站氣壓(hPa)\n",
       "0  2010/1/1 00:00   13.7     69.0      8.2     1021.8\n",
       "1  2010/1/1 01:00   13.8     69.0      8.3     1021.4\n",
       "2  2010/1/1 02:00   13.9     69.0      8.4     1020.4\n",
       "3  2010/1/1 03:00   13.9     69.0      8.4     1019.9\n",
       "4  2010/1/1 04:00   14.0     69.0      8.5     1019.6"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./station/466920taipei_train.csv')\n",
    "df = df[[\"觀測時間(hour)\", \"氣溫(℃)\", \"相對溼度(%)\", \"露點溫度(℃)\", \"測站氣壓(hPa)\"]]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((96360, 3), (96360, 1))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = df[[\"相對溼度(%)\", \"露點溫度(℃)\", \"測站氣壓(hPa)\"]]\n",
    "targets = df[[\"氣溫(℃)\"]].values\n",
    "\n",
    "features.shape, targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.27298851]\n",
      " [0.27586207]\n",
      " [0.27873563]\n",
      " ...\n",
      " [0.1954023 ]\n",
      " [0.20402299]\n",
      " [0.20402299]]\n",
      "(96360, 1)\n"
     ]
    }
   ],
   "source": [
    "mm = MinMaxScaler()\n",
    "ss = StandardScaler()\n",
    "\n",
    "features_trans = ss.fit_transform(features)\n",
    "targets_trans = mm.fit_transform(targets.reshape(-1, 1))\n",
    "\n",
    "print(targets_trans)\n",
    "print(targets_trans.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'pandas.core.frame.DataFrame'> <class 'numpy.ndarray'>\n",
      "(100, 3)\n"
     ]
    }
   ],
   "source": [
    "print(type(targets_trans))\n",
    "print(type(features), type(targets))\n",
    "print(features_trans[0:100].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96212, 100, 3) (96212, 50)\n"
     ]
    }
   ],
   "source": [
    "def split_sequences(input_sequences, output_sequence, n_steps_in, n_steps_out):\n",
    "    \n",
    "    features, targets = list(), list()\n",
    "    \n",
    "    for i in range(len(input_sequences)):\n",
    "        \n",
    "        # find the end of the input, output sequence\n",
    "        end_ix = i + n_steps_in\n",
    "        out_end_ix = end_ix + n_steps_out - 1\n",
    "\n",
    "        # check if we are beyond the dataset\n",
    "        if out_end_ix > len(input_sequences): break\n",
    "\n",
    "        # gather input and output of the pattern\n",
    "        # seq_x: features_trans[0~99](100個)\n",
    "        # seq_y: targets_trans[99~148:-1](50個)(-1: 2D(3D?)的最後一列)\n",
    "        seq_x, seq_y = input_sequences[i:end_ix], output_sequence[end_ix-1:out_end_ix, -1]  # 有-1才會是(96212,50)，沒有-1是(96212,50,1)\n",
    "        features.append(seq_x), targets.append(seq_y)\n",
    "\n",
    "    return np.array(features), np.array(targets)\n",
    "\n",
    "features_ss, targets_mm = split_sequences(features_trans, targets_trans, 100, 50)  # feed in 100 samples, up to the current day, and predict the next 50 time step values. \n",
    "print(features_ss.shape, targets_mm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.3591954 , 0.35344828, 0.33333333, 0.32183908, 0.31034483,\n",
       "       0.29022989, 0.28448276, 0.29310345, 0.29310345, 0.31609195,\n",
       "       0.32758621, 0.30747126, 0.30172414, 0.29310345, 0.28448276,\n",
       "       0.28448276, 0.27298851, 0.27011494, 0.27298851, 0.27586207,\n",
       "       0.27873563, 0.29022989, 0.29597701, 0.29885057, 0.29885057,\n",
       "       0.29310345, 0.29597701, 0.28735632, 0.29022989, 0.29597701,\n",
       "       0.29310345, 0.29022989, 0.29310345, 0.30172414, 0.3045977 ,\n",
       "       0.3045977 , 0.3045977 , 0.30172414, 0.29310345, 0.28448276,\n",
       "       0.2816092 , 0.2816092 , 0.2816092 , 0.28735632, 0.27586207,\n",
       "       0.27298851, 0.27011494, 0.27011494, 0.26724138, 0.27011494])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# assert: 斷言測試\n",
    "# [assert <test>, <message>]: test 是狀態測試，而 message 是斷言失敗時所要呈現訊息\n",
    "# assert targets_mm[0].all() == targets_trans[99:149].squeeze(1).all()\n",
    "\n",
    "targets_mm[0]  # 96212的第1個資料(50 column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.3591954 , 0.35344828, 0.33333333, 0.32183908, 0.31034483,\n",
       "       0.29022989, 0.28448276, 0.29310345, 0.29310345, 0.31609195,\n",
       "       0.32758621, 0.30747126, 0.30172414, 0.29310345, 0.28448276,\n",
       "       0.28448276, 0.27298851, 0.27011494, 0.27298851, 0.27586207,\n",
       "       0.27873563, 0.29022989, 0.29597701, 0.29885057, 0.29885057,\n",
       "       0.29310345, 0.29597701, 0.28735632, 0.29022989, 0.29597701,\n",
       "       0.29310345, 0.29022989, 0.29310345, 0.30172414, 0.3045977 ,\n",
       "       0.3045977 , 0.3045977 , 0.30172414, 0.29310345, 0.28448276,\n",
       "       0.2816092 , 0.2816092 , 0.2816092 , 0.28735632, 0.27586207,\n",
       "       0.27298851, 0.27011494, 0.27011494, 0.26724138, 0.27011494])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets_trans[99:149].squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Shape: (96062, 100, 3) (96062, 50)\n",
      "Testing Shape: (150, 100, 3) (150, 50)\n"
     ]
    }
   ],
   "source": [
    "total_samples = len(features)\n",
    "train_test_split = round(0.7 * total_samples)\n",
    "\n",
    "features_train = features_ss[:-150]\n",
    "features_test = features_ss[-150:]\n",
    "\n",
    "targets_train = targets_mm[:-150]\n",
    "targets_test = targets_mm[-150:]\n",
    "\n",
    "print(\"Training Shape:\", features_train.shape, targets_train.shape)\n",
    "print(\"Testing Shape:\", features_test.shape, targets_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "# convert to pytorch tensors\n",
    "features_train_tensors = Variable(torch.Tensor(features_train))\n",
    "features_test_tensors = Variable(torch.Tensor(features_test))\n",
    "\n",
    "targets_train_tensors = Variable(torch.Tensor(targets_train))\n",
    "targets_test_tensors = Variable(torch.Tensor(targets_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Shape: torch.Size([96062, 100, 3]) torch.Size([96062, 50])\n",
      "Testing Shape: torch.Size([150, 100, 3]) torch.Size([150, 50])\n"
     ]
    }
   ],
   "source": [
    "# reshaping to rows, timestamps, features\n",
    "# batch_size, sequence_length, input size (the number of features)\n",
    "features_train_tensors_final = torch.reshape(features_train_tensors, \n",
    "                                             (features_train_tensors.shape[0], 100, features_train_tensors.shape[2]))\n",
    "features_test_tensors_final = torch.reshape(features_test_tensors,\n",
    "                                            (features_test_tensors.shape[0], 100, features_test_tensors.shape[2])) \n",
    "\n",
    "print(\"Training Shape:\", features_train_tensors_final.shape, targets_train_tensors.shape)\n",
    "print(\"Testing Shape:\", features_test_tensors_final.shape, targets_test_tensors.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_check, targets_check = split_sequences(features, targets.reshape(-1, 1), 100, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_classes, input_size, hidden_size, num_layers):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes # output size\n",
    "        self.num_layers = num_layers # number of recurrent layers in the lstm\n",
    "        self.input_size = input_size # input size\n",
    "        self.hidden_size = hidden_size # neurons in each lstm layer\n",
    "        # LSTM model\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
    "                            num_layers=num_layers, batch_first=True, dropout=0.2) # lstm\n",
    "        self.fc_1 =  nn.Linear(hidden_size, 128) # fully connected \n",
    "        self.fc_2 = nn.Linear(128, num_classes) # fully connected last layer\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        # hidden state\n",
    "        h_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size))\n",
    "        # cell state\n",
    "        c_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size))\n",
    "        # propagate input through LSTM\n",
    "        output, (hn, cn) = self.lstm(x, (h_0, c_0)) # (input, hidden, and internal state)\n",
    "        hn = hn.view(-1, self.hidden_size) # reshaping the data for Dense layer next\n",
    "        out = self.relu(hn)\n",
    "        out = self.fc_1(out) # first dense\n",
    "        out = self.relu(out) # relu\n",
    "        out = self.fc_2(out) # final output\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "n_epochs = 1000 # 1000 epochs\n",
    "learning_rate = 0.001 # 0.001 lr\n",
    "\n",
    "input_size = 4 # number of features\n",
    "hidden_size = 2 # number of features in hidden state\n",
    "num_layers = 1 # number of stacked lstm layers\n",
    "\n",
    "num_classes = 50 # number of output classes \n",
    "\n",
    "lstm = LSTM(num_classes, \n",
    "              input_size, \n",
    "              hidden_size, \n",
    "              num_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, lstm, optimiser, loss_fn, X_train, y_train,\n",
    "                  X_test, y_test):\n",
    "    for epoch in range(n_epochs):\n",
    "        lstm.train()\n",
    "        outputs = lstm.forward(X_train) # forward pass\n",
    "        optimiser.zero_grad() # calculate the gradient, manually setting to 0\n",
    "        # obtain the loss function\n",
    "        loss = loss_fn(outputs, y_train)\n",
    "        loss.backward() # calculates the loss of the loss function\n",
    "        optimiser.step() # improve from loss, i.e backprop\n",
    "        # test loss\n",
    "        lstm.eval()\n",
    "        test_preds = lstm(X_test)\n",
    "        test_loss = loss_fn(test_preds, y_test)\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch: %d, train loss: %1.5f, test loss: %1.5f\" % (epoch, \n",
    "                                                                      loss.item(), \n",
    "                                                                      test_loss.item())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'optimiser' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m training_loop(n_epochs\u001b[38;5;241m=\u001b[39mn_epochs,\n\u001b[0;32m      2\u001b[0m               lstm\u001b[38;5;241m=\u001b[39mlstm,\n\u001b[1;32m----> 3\u001b[0m               optimiser\u001b[38;5;241m=\u001b[39m\u001b[43moptimiser\u001b[49m,\n\u001b[0;32m      4\u001b[0m               loss_fn\u001b[38;5;241m=\u001b[39mloss_fn,\n\u001b[0;32m      5\u001b[0m               X_train\u001b[38;5;241m=\u001b[39mX_train_tensors_final,\n\u001b[0;32m      6\u001b[0m               y_train\u001b[38;5;241m=\u001b[39my_train_tensors,\n\u001b[0;32m      7\u001b[0m               X_test\u001b[38;5;241m=\u001b[39mX_test_tensors_final,\n\u001b[0;32m      8\u001b[0m               y_test\u001b[38;5;241m=\u001b[39my_test_tensors)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'optimiser' is not defined"
     ]
    }
   ],
   "source": [
    "training_loop(n_epochs=n_epochs,\n",
    "              lstm=lstm,\n",
    "              optimiser=optimiser,\n",
    "              loss_fn=loss_fn,\n",
    "              X_train=X_train_tensors_final,\n",
    "              y_train=y_train_tensors,\n",
    "              X_test=X_test_tensors_final,\n",
    "              y_test=y_test_tensors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
